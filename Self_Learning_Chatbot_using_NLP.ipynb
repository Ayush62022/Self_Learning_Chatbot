{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b5812f",
   "metadata": {},
   "source": [
    "# Import Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07722448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde53c36",
   "metadata": {},
   "source": [
    "# Reading the corpus of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cd74f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.txt','r',errors = 'ignore')\n",
    "raw_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c85aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\AYUSH NATH\n",
      "[nltk_data]     TIWARI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\AYUSH NATH\n",
      "[nltk_data]     TIWARI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\AYUSH NATH\n",
      "[nltk_data]     TIWARI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "raw_doc = raw_doc.lower() # converting entire text to lower case\n",
    "nltk.download('punkt') # using the punkt tokenizer\n",
    "nltk.download('wordnet') # using the wordnet dictionary\n",
    "nltk.download('omw-1.4')\n",
    "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0d96b",
   "metadata": {},
   "source": [
    ">Punkt Sentence Tokenizer\n",
    "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "\n",
    ">WordNet is an English dictionary which is a part of Natural Language Tool Kit (NLTK) for Python. This is an extensive library built to make Natural Language Processing (NLP) easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedab5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmain menu\\n\\nwikipedia the free encyclopedia\\n\\n    create account\\n    log in\\n\\npersonal tools\\n\\ncontents\\n\\n    beginning\\n    how does ai work?\\n    history\\n    related pages\\n    references\\n\\nartificial intelligence\\n\\n    page\\n    talk\\n\\n    read\\n    change\\n    change source\\n    view history\\n\\ntools\\n\\nfrom simple english wikipedia, the free encyclopedia\\n\\t\\nthis article needs to be updated. you can help wikipedia by updating it. (may 2023)\\n\\nartificial intelligence (ai) is the ability of a computer program or a machine to think and learn.[1] it is also a field of study which tries to make computers \"smart\". they work on their own without being encoded with commands. john mccarthy came up with the name, \"artificial intelligence\" in 1955.\\n\\nin general use, the term \"artificial intelligence\" means a programme which mimics human cognition. at least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do.[2] andreas kaplan and michael haenlein define ai as a systemâ€™s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.[3]\\n\\nan ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective.[4] as machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. for example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.\\n\\nat present we use the term ai for successfully understanding human speech,[2] competing at a high level in strategic game systems (such as chess and go), self-driving cars, and interpreting complex data.[5] some people also consider ai a danger to humanity if it continues to progress at its current pace.[6]\\n\\nan extreme goal of ai research is to create computer programs that can learn, solve problems, and think logically.[7][8] in practice, however, most applications have picked on problems which computers can do well. searching databases and doing calculations are things computers do better than people. on the other hand, \"perceiving its environment\" in any real sense is way beyond present-day computing.\\n\\nai involves many different fields like computer science, mathematics, linguistics, psychology, neuroscience, and philosophy. eventually researchers hope to create a \"general artificial intelligence\" which can solve many problems instead of focusing on just one. researchers are also trying to create creative and emotional ai which can possibly empathize or create art. many approaches and tools have been tried.\\n\\nborrowing from the management literature, kaplan and haenlein classify artificial intelligence into three different types of ai systems: analytical, human-inspired, and humanized artificial intelligence.[3] analytical ai has only characteristics consistent with cognitive intelligence generating cognitive representation of the world and using learning based on past experience to inform future decisions. human-inspired ai has elements from cognitive as well as emotional intelligence, understanding, in addition to cognitive elements, also human emotions considering them in their decision making. humanized ai shows characteristics of all types of competencies (i.e., cognitive, emotional, and social intelligence), able to be self-conscious and self-aware in interactions with others.[9]\\nhow does ai work?\\n\\nai works by utilizing algorithms and models to process data and perform intelligent tasks. here\\'s a more detailed explanation of how ai works:\\n\\ndata collection:\\n\\nai systems require a large amount of data to learn and make predictions. data can be collected from various sources such as databases, sensors, or the internet.\\n\\ndata preprocessing:\\n\\nthe collected data is cleaned, normalized, and transformed to remove noise, handle missing values, and ensure it is in a suitable format for analysis.\\n\\nfeature extraction:\\n\\nrelevant features or characteristics are extracted from the preprocessed data. this step helps in representing the data in a way that is suitable for the ai algorithms.\\n\\nalgorithm selection:\\n\\ndifferent ai algorithms can be chosen depending on the nature of the problem. standard algorithms include machine learning algorithms like decision trees, random forests, support vector machines, or deep learning algorithms like neural networks.\\n\\nmodel training:\\n\\nthe selected algorithm is trained using the preprocessed data. during training, the algorithm learns patterns and relationships in the data to make predictions or perform specific tasks. the training process involves adjusting the algorithm\\'s internal parameters based on the provided data.\\n\\nmodel evaluation: the trained model is evaluated on a separate dataset, called the validation or test set, to assess its performance. various metrics are used to measure accuracy, precision, recall, or other relevant performance indicators.\\n\\nmodel optimization:\\n\\nif the model\\'s performance is not satisfactory, optimization techniques like hyperparameter tuning or regularization can be applied to improve its effectiveness. this step involves fine-tuning the model\\'s parameters to enhance its performance.\\n\\ndeployment:\\n\\nonce the model achieves the desired performance, it can be deployed to perform tasks on new, unseen data. the ai system can make predictions, classify objects, generate recommendations, or perform other specific tasks based on its training.\\n\\ncontinuous learning and improvement:\\n\\nai systems can be designed to learn from new data and adapt to changing conditions. by continually updating and retraining the models, ai systems can improve their performance over time.\\n\\nit\\'s important to note that the specific details and techniques involved in ai can vary depending on the subfield, approach, and algorithms being used. ai is a vast field with ongoing research and advancements, and the above steps provide a general framework for understanding how ai systems work.[10][11]\\n\\ntypes of artificial intelligence:\\n\\nreactive ai:\\n\\nreactive ai systems are designed to react to current situations without any memory or ability to learn from past experiences. they excel in specific tasks and are commonly used in areas such as gaming and autonomous vehicles. reactive ai does not possess the ability to understand context or exhibit memory.\\n\\nlimited memory ai:\\n\\nlimited memory ai systems have the capability to retain some past information and use it to make decisions. these systems incorporate historical data or observations to enhance their responses and performance. applications of limited memory ai include virtual assistants and recommendation systems.\\n\\ntheory of mind ai:\\n\\ntheory of mind ai aims to develop machines that can understand and attribute mental states to themselves and others. this type of ai involves perceiving emotions, intentions, beliefs, and desires of other entities to interact and make informed decisions. theory of mind ai has potential applications in social robotics and human-computer interaction.\\n\\nself-aware ai:\\n\\nself-aware ai represents machines that have consciousness and a sense of their own existence. while this concept is more theoretical, researchers explore the possibility of developing ai systems with self-awareness. self-aware ai raises profound philosophical questions and could have implications for the future of ai and ethics.\\n\\nnarrow ai:\\n\\nnarrow ai, also known as weak ai, focuses on performing specific tasks or solving specific problems. these ai systems excel in a particular domain, such as image recognition or natural language processing. examples of narrow ai include voice assistants like siri and alexa, as well as recommendation algorithms.\\n\\ngeneral ai:\\n\\ngeneral ai, also referred to as strong ai, aims to replicate human-level intelligence across various domains. these ai systems possess the ability to understand, learn, and apply knowledge in diverse situations. general ai is still an area of ongoing research and represents the pinnacle of ai development.\\n\\nsuperintelligent ai:\\n\\nsuperintelligent ai refers to ai systems that surpass human intelligence in nearly all aspects. these hypothetical systems possess advanced cognitive abilities and can outperform humans in intellectual tasks. superintelligent ai is a topic of speculation and raises important questions regarding its impact on society.[12]\\nhistory\\n\\nai research really started with a conference at dartmouth college in 1956. it was a month-long brainstorming session attended by many people with interests in ai. at the conference they wrote programs that were amazing at the time, beating people at checkers or solving word problems. the department of defense started giving a lot of money to ai research and labs were created all over the world.\\n\\nunfortunately, researchers seriously undervalued how challenging several issues were. they still couldn\\'t offer computers things like emotions or common sense using the techniques they had employed. in a paper on ai, mathematician james lighthill stated that \"no aspect of the discipline has so far seen discoveries generated the huge influence that was previously anticipated.\" the governments of the us and uk desired to support more profitable initiatives. a \"ai winter\" in which little research was conducted was brought on by cuts\\n\\nai revived again in the 90s and early 2000s with its use in data mining and medical diagnosis. this was possible because of faster computers and focusing on solving more specific problems. in 1997, deep blue became the first computer program to beat chess world champion garry kasparov. faster computers, advances in deep learning, and access to more data have made ai popular throughout the world.[13] in 2011 ibm watson beat the top two jeopardy! players brad rutter and ken jennings, and in 2016 google\\'s alphago beat top go player lee sedol 4 out of 5 times.\\nrelated pages\\n\\n    artificial neural network\\n    neural networks\\n    expert systems\\n    machine learning\\n\\nreferences\\n\\n\"andreas kaplan, artificial intelligence, business and civilization: our fate made in machines, routledge, 2022\".\\nrussell, stuart j. & norvig, peter 2003. artificial intelligence: a modern approach. 2nd ed, upper saddle river, new jersey: prentice hall. isbn 0-13-790395-2\\nkaplan, andreas; haenlein, michael (january 2019). \"siri, siri, in my hand: who\\'s the fairest in the land? on the interpretations, illustrations, and implications of artificial intelligence\". business horizons. 62 (1): 15â€“25. doi:10.1016/j.bushor.2018.08.004. s2cid 158433736.\\nhutter, marcus 2005. universal artificial intelligence. berlin: springer. isbn 978-3-540-22139-5\\nnilsson, nils 1998. artificial intelligence: a new synthesis. morgan kaufmann. isbn 978-1-55860-467-4\\n\"stephen hawking believes ai could be mankind\\'s last accomplishment\". betanews. 21 october 2016.\\nkurzweil, ray 1999. the age of spiritual machines. penguin books. isbn 0-670-88217-8.\\nkurzweil, ray 2005. the singularity is near. viking press\\n\"artificial intelligence: more than a natural intelligence?\". 16 november 2019.\\nknowledgecafe0.blogspot.com https://knowledgecafe0.blogspot.com/2023/06/what-is-ai-and-how-it-work.html. retrieved 2023-06-17. {{cite web}}: missing or empty |title= (help)\\ncafe, knowledge (6/17/2023). \"what is ai and how it work\". knowledgecafã©. retrieved 6/17/2023. {{cite web}}: check date values in: |access-date= and |date= (help)\\nvyas, dilraj (2023-06-09). \"artificial intelligence - techbuzzblog\". retrieved 2023-06-09.\\n\\n    kaplan, andreas; haenlein, michael (2020). \"rulers of the world, unite! the challenges and opportunities of artificial intelligence\". business horizons. 63: 37â€“50. doi:10.1016/j.bushor.2019.09.003. s2cid 211456730.\\n\\nauthority control: national edit this at wikidata\\t\\n\\n    spain france bnf data germany israel united states latvia japan czech republic\\n\\ncategory:\\n\\n    artificial intelligence\\n\\n    this page was last changed on 17 june 2023, at 15:55.\\n    text is available under the creative commons attribution/share-alike license and the gfdl; additional terms may apply. see terms of use for details.\\n\\n    privacy policy\\n    about wikipedia\\n    disclaimers\\n    code of conduct\\n    mobile view\\n    developers\\n    statistics\\n    cookie statement\\n\\n    wikimedia foundation\\n    powered by mediawiki\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "raw_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a62936",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f623927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0943e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'john mccarthy came up with the name, \"artificial intelligence\" in 1955.\\n\\nin general use, the term \"artificial intelligence\" means a programme which mimics human cognition.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef82f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikipedia', 'the', 'free', 'encyclopedia', 'create', 'account']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_tokens[2:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0a120",
   "metadata": {},
   "source": [
    "## Performing Text Pre-Processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab6715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punc_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ff8b8",
   "metadata": {},
   "source": [
    ">Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word.\n",
    "\n",
    ">The process of removing affixes from a word so that we are left with the stem of that word is called stemming. For example, consider the words 'run', 'running', and 'runs', all convert into the root word 'run' after stemming is implemented on them\n",
    "\n",
    ">lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n",
    "\n",
    ">Morphology focuses on how the components within a word (stems, root words, prefixes, suffixes, etc.) are arranged or modified to create different meanings. English, for example, often adds \"-s\" or \"-es\" to the end of count nouns to indicate plurality, and a \"-d\" or \"-ed\" to a verb to indicate past tense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261b1e4",
   "metadata": {},
   "source": [
    "## Define Greeting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ae7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_inputs = (\"hello\",\"hi\",\"whassup\",\"how are you?\")\n",
    "greet_responses = (\"hi\",\"Hey\",\"Hey There!\",\"There there!!\")\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d854f",
   "metadata": {},
   "source": [
    "## Response Generation by the Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1950d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f90bc0",
   "metadata": {},
   "source": [
    ">CountVectorizer simply counts the number of times a word appears in a document (using a bag-of-words approach), while TF-IDF Vectorizer takes into account not only how many times a word appears in a document but also how important that word is to the whole corpus.\n",
    "\n",
    ">The TfidfVectorizer uses an in-memory vocabulary (a python dict) to map the most frequent words to feature indices and hence compute a word occurrence frequency (sparse) matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7e06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo1_response = ' '\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten() # flatten combines....items and lists to be combined into a single list in flatten.\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if (req_tfidf == 0):\n",
    "        robo1_response = robo1_response + \"I am sorry. Unable to understand you!\"\n",
    "        return robo1_response\n",
    "    else:\n",
    "        robo1_response = robo1_response + sentence_tokens[idx]\n",
    "        return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84525b79",
   "metadata": {},
   "source": [
    ">Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cdb8be",
   "metadata": {},
   "source": [
    "## Defining the Chat Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "497ce0df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!I am  Retreival Learning Bot. Start typing your text after greeting to talk to me.For ending conversation type bye!\n",
      "hello\n",
      "Bot hi\n",
      "Tell me about yourself\n",
      "Bot: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AYUSH NATH TIWARI\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH NATH TIWARI\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am sorry. Unable to understand you!\n",
      "Tell me about genral AI\n",
      "Bot:  \"what is ai and how it work\".\n",
      "ok\n",
      "Bot:  I am sorry. Unable to understand you!\n",
      "bye\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print('Hello!I am  Retreival Learning Bot. Start typing your text after greeting to talk to me.For ending conversation type bye!')\n",
    "while(flag == True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response != 'bye'):\n",
    "        if(user_response == 'thank you' or user_response == 'thanks'):\n",
    "            flag = False\n",
    "            print('Bot: You are Welcome..')\n",
    "        else:\n",
    "            if(greet(user_response) != None):\n",
    "                print('Bot ' + greet(user_response))\n",
    "            else:\n",
    "                sentence_tokens.append(user_response)\n",
    "                word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
    "                final_words = list(set(word_tokens))\n",
    "                print('Bot: ',end = '')\n",
    "                print(response(user_response))\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print('Bot: Goodbye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052512c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
